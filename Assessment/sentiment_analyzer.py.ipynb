{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add9fb91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_nvidia_ai_endpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_nvidia_ai_endpoints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatNVIDIA\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_nvidia_ai_endpoints'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Load emails (assumes a local emails.json file)\n",
    "with open('data/emails.json', 'r') as f:\n",
    "    emails = json.load(f)\n",
    "\n",
    "# Format emails into a single string\n",
    "email_text = \"\\n\\n\".join(emails)\n",
    "\n",
    "# LangChain-compatible LLM (LLaMA 3 via NVIDIA endpoint)\n",
    "llm = ChatNVIDIA(\n",
    "    base_url=\"http://llama:8000/v1\", \n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Prompt Template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant analyzing customer sentiment from emails. \n",
    "Your task is to determine:\n",
    "\n",
    "1. The product category with the most negative sentiment (e.g., clothing, furniture, electronics).\n",
    "2. The store location with the highest number of customer complaints.\n",
    "\n",
    "Respond concisely and in this exact format:\n",
    "\n",
    "The product category with the most negative sentiment is {{category}}.  \n",
    "The store location with the most customer complaints is {{location}}.\n",
    "\n",
    "Here are the customer emails:\n",
    "{emails}\n",
    "\"\"\")\n",
    "\n",
    "# Output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Build the chain\n",
    "chain: Runnable = prompt | llm | parser\n",
    "\n",
    "# Run the chain with the emails\n",
    "result = chain.invoke({\"emails\": email_text})\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
